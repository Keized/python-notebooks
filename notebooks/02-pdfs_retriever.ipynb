{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65880853-35c5-4d39-8b60-15e4e9ef1a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48d34513-63f1-4aaa-816b-ecdf6813e115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from typing import (List)\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "splitter = RecursiveCharacterTextSplitter()\n",
    "\n",
    "documents = []\n",
    "loaders = [\n",
    "    PyPDFLoader(\"../data/autogpt.pdf\"),\n",
    "    PyPDFLoader(\"../data/lora.pdf\"),\n",
    "]\n",
    "\n",
    "for loader in loaders:\n",
    "    documents.extend(loader.load())\n",
    "\n",
    "chunks = splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7767714",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = '../docs/chroma/02'\n",
    "vectordb = Chroma(\n",
    "    embedding_function=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "if (len(vectordb.get()['ids']) < 1):\n",
    "    print('persist...')\n",
    "    vectordb.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embedding,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "554feb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': 5, 'source': '../data/autogpt.pdf'}\n",
      "++++\n",
      "{'page': 5, 'source': '../data/autogpt.pdf'}\n",
      "\n",
      "\n",
      "{'page': 2, 'source': '../data/lora.pdf'}\n",
      "++++\n",
      "{'page': 4, 'source': '../data/autogpt.pdf'}\n",
      "\n",
      "\n",
      "{'page': 4, 'source': '../data/lora.pdf'}\n",
      "++++\n",
      "{'page': 2, 'source': '../data/autogpt.pdf'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Working with metadata\n",
    "question = \"what about the persistence boost\"\n",
    "docs = vectordb.similarity_search(question,k=3)\n",
    "auto_gpt_docs = vectordb.similarity_search(question,k=3, filter={\"source\": \"../data/autogpt.pdf\"})\n",
    "\n",
    "for index in range(3):\n",
    "    print(docs[index].metadata)\n",
    "    print('++++')\n",
    "    print(auto_gpt_docs[index].metadata)\n",
    "    print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5ac9a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with metadata using selfquery\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"source\",\n",
    "        description=\"The paper the chunk is from, should be one of `../data/autogpt.pdf`, `../data/lora.pdf`\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"page\",\n",
    "        description=\"The page from the paper\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "llm = OpenAI(model='gpt-3.5-turbo-instruct', temperature=0)\n",
    "\n",
    "document_content_description = \"LLM Papers\"\n",
    "\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vectordb,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "979fb840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4 Additional opini\n",
      "{'page': 3, 'source': '../data/autogpt.pdf'}\n",
      "such as self-consist\n",
      "{'page': 0, 'source': '../data/autogpt.pdf'}\n",
      "to enhance its decis\n",
      "{'page': 5, 'source': '../data/autogpt.pdf'}\n",
      "and ALFWorld experim\n",
      "{'page': 5, 'source': '../data/autogpt.pdf'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = 'What did they say about methodology in the autogpt paper page 1 ?' # cause error\n",
    "query = 'What did they say about methodology in the autogpt paper ?' \n",
    "\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "for d in docs:\n",
    "    print(d.page_content[:20])\n",
    "    print(d.metadata)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6c59f39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))\n",
    "\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectordb.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "16c238c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "\"Our experimental results highlight the successful adaptation\n",
      "of the Auto-GPT styled agent to complex online decision-\n",
      "making tasks through straightforward prompt design, sur-\n",
      "passing IL-based baseline models specifically designed for\n",
      "these tasks. Among the foundational LLMs powering Auto-\n",
      "GPT, GPT-4 demonstrates superior performance. Addition-\n",
      "ally, we introduce an innovative strategy of incorporat-\n",
      "ing additional opinions from external expert models, fur-\n",
      "ther enhancing the decision-making capabilities of Auto-\n",
      "GPT styled agents, particularly benefiting GPT-4. Our Addi-\n",
      "tional Opinions algorithm provides a lightweight supervised\n",
      "training approach for Auto-GPT styled agents, enabling\n",
      "improved performance without requiring extensive fine-\n",
      "tuning of the LLMs.\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "- \"2 METHODOLOGY\"\n",
      "- \"2.1 Tasks and baseline models\"\n",
      "- \"2.1.1 WebShop. Webshop [ 24] is a simulated environment\"\n",
      "- \"replicates a web shopping experience by scraping 1,181,436 products from Amazon.com and hosting them on an isolated server.\"\n",
      "- \"The environment offers agents a realistic action space, including options such as performing product searches, clicking on items, navigating back to previous pages, and making purchases.\"\n",
      "- \"Equipped with an integrated search engine, the environment provides the shopping agent with real-time observations that mimic those from a web browser.\"\n",
      "- \"The evaluation process involves determining whether the agent successfully purchases the intended product based on the following question and context.\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "opinions’ provided to Auto-GPT consistently used the supe-\n",
      "rior IL model with image access. The temperature was set to\n",
      "0.01 across all models to reduce randomness. Evaluation of\n",
      "IL models and Auto-GPT + IL variants followed a rigorous\n",
      "protocol to mitigate sampling randomness and assess small\n",
      "observed variations, respectively. In the case of Auto-GPT\n",
      "alone, we performed a single run due to the minimal vari-\n",
      "ations observed as a result of the low temperature setting.\n",
      "Nonetheless, small variations were noticed in the Auto-GPT\n",
      "+ IL variants, prompting us to conduct two runs and use\n",
      "their average for analysis.\n",
      "In the original research paper, a supervised Imitation\n",
      "Learning (IL) model was trained to aid the agent in making\n",
      "optimal decisions at each stage, with the ultimate goal of\n",
      "executing the correct purchase based on a given human in-\n",
      "struction. The system was structured around four principal\n",
      "tasks: (1) generating a quality search query based on the\n",
      "original instruction, (2) selecting an item to browse from\n",
      "a variety of items and their titles, (3) deciding whether to\n",
      "check ’Description’, ’Features’ or ’Reviews’ in the product\n",
      "detail page while also making the\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "- Auto-GPT is an autonomous agent that leverages recent advancements in adapting Large Language Models (LLMs) for decision-making tasks.\n",
      "- In this paper, we present a comprehensive benchmark study of Auto-GPT styled agents in decision-making tasks that simulate real-world scenarios.\n",
      "- We compare the performance of popular LLMs such as GPT-4, GPT-3.5, Claude, and Vicuna in Auto-GPT styled decision-making tasks.\n",
      "- Furthermore, we introduce the Additional Opinions algorithm, an easy and effective method that incorporates supervised/imitation-based learners into the Auto-GPT scheme.\n",
      "- We demonstrate through careful baseline comparisons and ablation studies that the Additional Opinions algorithm significantly enhances performance in online decision-making benchmarks, including WebShop and ALFWorld.\n",
      "- Adapting Large Language Models (LLM) to autonomous agents have recently achieved great success in various decision-making[ 3], virtual character simulation[ 12] and tool manipulation tasks[ 9].\n",
      "- We characterize Auto-GPT styled agent with the following properties (1) it receives only high-level goals and instructions at the beginning of complex multi-step tasks, without requiring step-by-step guidance from humans (2) it engages in self-monologue by generating 'Thoughts,' '\n"
     ]
    }
   ],
   "source": [
    "query = 'What did they say about methodology in the autogpt paper ?' \n",
    "compressed_docs = compression_retriever.get_relevant_documents(query)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
