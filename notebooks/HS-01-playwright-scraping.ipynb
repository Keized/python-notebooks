{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping https://lespotevry.fr at depth 0\n",
      "Not narrative. Text does not contain a verb:\n",
      "\n",
      "Aujourd'hui, votre centre est ouvert jusqu'à 19:00\n",
      "Not narrative. Text does not contain a verb:\n",
      "\n",
      "Aujourd'hui votre hypermarché est ouvert jusqu'à 19:00\n",
      "Not narrative. Text does not contain a verb:\n",
      "\n",
      "Vos restaurants sont ouverts jusqu'à 23:00\n",
      "Sentence does not exceed 3 word tokens, it will not count toward sentence count.\n",
      "cest quoi \n",
      "Sentence does not exceed 3 word tokens, it will not count toward sentence count.\n",
      "cest quoi \n",
      "Not narrative. Text does not contain a verb:\n",
      "\n",
      "c'est quoi ?\n",
      "Sentence does not exceed 5 word tokens, it will not count toward sentence count.\n",
      "cest quoi \n",
      "Not narrative. Text does not contain a verb:\n",
      "\n",
      "Pour tous nos bons plans, c'est par ici !\n",
      "Sentence does not exceed 3 word tokens, it will not count toward sentence count.\n",
      "Facebook\n",
      "Not narrative. Text exceeds cap ratio 0.5:\n",
      "\n",
      "Facebook\n",
      "Sentence does not exceed 5 word tokens, it will not count toward sentence count.\n",
      "Facebook\n",
      "Sentence does not exceed 3 word tokens, it will not count toward sentence count.\n",
      "Instagram\n",
      "Not narrative. Text exceeds cap ratio 0.5:\n",
      "\n",
      "Instagram\n",
      "Sentence does not exceed 5 word tokens, it will not count toward sentence count.\n",
      "Instagram\n",
      "Sentence does not exceed 3 word tokens, it will not count toward sentence count.\n",
      "Tiktok\n",
      "Not narrative. Text exceeds cap ratio 0.5:\n",
      "\n",
      "Tiktok\n",
      "Sentence does not exceed 5 word tokens, it will not count toward sentence count.\n",
      "Tiktok\n",
      "Sentence does not exceed 3 word tokens, it will not count toward sentence count.\n",
      "Newsletter\n",
      "Not narrative. Text exceeds cap ratio 0.5:\n",
      "\n",
      "Newsletter\n",
      "Sentence does not exceed 5 word tokens, it will not count toward sentence count.\n",
      "Newsletter\n",
      "Sentence does not exceed 3 word tokens, it will not count toward sentence count.\n",
      "Youtube\n",
      "Not narrative. Text exceeds cap ratio 0.5:\n",
      "\n",
      "Youtube\n",
      "Sentence does not exceed 5 word tokens, it will not count toward sentence count.\n",
      "Youtube\n",
      "[Document(metadata={'source': 'https://lespotevry.fr'}, page_content=\"Aujourd'hui, votre centre est ouvert jusqu'à 19:00\\n\\nAujourd'hui votre hypermarché est ouvert jusqu'à 19:00\\n\\nVos restaurants sont ouverts jusqu'à 23:00\\n\\nLe Spot\\n\\nc'est quoi ?\\n\\nLe Spot, c’est votre nouveau terrain de jeu shopping, food, loisirs et culture au cœur d’Evry-Courcouronnes.\\xa0126\\xa0000 m²\\xa0réunissant pour la 1ère fois en France, des commerces, des restaurants, un cinéma, une médiathèque, un théâtre, une salle de spectacle, une piscine et une patinoire.\\xa0 Le Spot, un hyper lieu vivant, animé, végétalisé et ouvert sur son environnement. Pour shopper, rire, se régaler, se balader, chiller ou se dépenser, Le Spot, c’est une nouvelle destination à découvrir en famille, entre les amis, ou seul, tous les jours même le dimanche et en soirée.\\n\\nÇa se passe en ce moment\\n\\n            Soldes d'été du 26 juin au 23 juillet\\n\\nÇa se passe en ce moment\\n\\n            Vivez l'Euro de foot 2024 au Spot !\\n\\nÇa se passe en ce moment\\n\\n            Les Olympiades du Spot du 5 au 21 juillet\\n\\nBESSON CHAUSSURES\\n\\nLa Casa de las Carcasas\\n\\nPour tous nos bons plans, c'est par ici !\\n\\nInscrivez-vous à notre newsletter pour recevoir toutes les actualités,\\n\\nBons plans, et bonnes idées\\n\\nFacebook\\n\\nInstagram\\n\\nTiktok\\n\\nNewsletter\\n\\nYoutube\")]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from playwright.async_api import async_playwright\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from unstructured.partition.html import partition_html\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "selectors_to_ignore = [\n",
    "    'iframe', 'head',\n",
    "    '.ad', '.ads', '.advertisement', '.banner',\n",
    "    'nav', 'footer', 'header',\n",
    "    '.navbar', '.menu', '.nav', '.footer', '.header',\n",
    "    'form', 'button', 'input', 'select', 'textarea',\n",
    "    '.form', '.button', '.btn',\n",
    "    '.widget', '.social', '.share', '.tweet', '.like',\n",
    "    'script', 'noscript', 'style', 'link',\n",
    "    '.comments', '.comment', '.reply', '.discussion',\n",
    "    '.forum', '.thread'\n",
    "]\n",
    "\n",
    "logger = logging.getLogger(\"webscapper.py\")\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout,  # uncomment this line to redirect output to console\n",
    "    format=\"%(message)s\",\n",
    "    level=logging.DEBUG,\n",
    ")\n",
    "\n",
    "visited_urls = set()\n",
    "internal_urls = set()\n",
    "documents = []\n",
    "\n",
    "def save_func(url, content):\n",
    "    metadata = {\"source\": url}\n",
    "    documents.append(Document(page_content=content, metadata=metadata))\n",
    "\n",
    "def clean_content(source):\n",
    "    elements = partition_html(text=source)\n",
    "    clean_content =  \"\\n\\n\".join([str(el) for el in elements])\n",
    "    return clean_content\n",
    "\n",
    "def is_internal_url(base_url, test_url):\n",
    "    base_netloc = urlparse(base_url).netloc\n",
    "    test_netloc = urlparse(test_url).netloc\n",
    "    return base_netloc == test_netloc\n",
    "\n",
    "\n",
    "async def scrape_page(page, url, process_content_func, current_depth, max_depth):\n",
    "    if current_depth > max_depth:\n",
    "        return\n",
    "    \n",
    "    logger.info(f\"Scraping {url} at depth {current_depth}\")\n",
    "    await page.goto(url)\n",
    "    await page.wait_for_load_state(\"networkidle\")\n",
    "\n",
    "    ignore_script = f'''\n",
    "    (() => {{\n",
    "        const selectors = {selectors_to_ignore};\n",
    "        selectors.forEach(selector => {{\n",
    "            const elements = document.querySelectorAll(selector);\n",
    "            elements.forEach(element => element.remove());\n",
    "        }});\n",
    "    }})()'''\n",
    "\n",
    "    await page.evaluate(ignore_script)\n",
    "    content = await page.content()\n",
    "    content = clean_content(content)\n",
    "    process_content_func(url, content)\n",
    "\n",
    "    links_locator = await page.get_by_role('link').all()\n",
    "    for link in links_locator:\n",
    "        href = await link.get_attribute('href')\n",
    "        if href:\n",
    "            full_url = urljoin(url, href)\n",
    "            if is_internal_url(url, full_url):\n",
    "                internal_urls.add((full_url, current_depth + 1))\n",
    "\n",
    "async def process_url(playwright, url, process_content_func, current_depth, max_depth):\n",
    "    browser = await playwright.chromium.launch(headless=True)\n",
    "    page = await browser.new_page()\n",
    "    try:\n",
    "        await scrape_page(page, url, process_content_func, current_depth, max_depth)\n",
    "    except Exception as ex:\n",
    "        logger.error(f\"Error scraping {url}: {str(ex)}\")\n",
    "    finally:\n",
    "        await browser.close()\n",
    "\n",
    "async def main(url, max_depth=2, max_pages=100, concurrency=5):\n",
    "    internal_urls.add((url, 0))\n",
    "    async with async_playwright() as playwright:\n",
    "        page_count = 0\n",
    "        while internal_urls and page_count < max_pages:\n",
    "            next_url, depth = internal_urls.pop()\n",
    "            if next_url not in visited_urls:\n",
    "                await process_url(playwright, next_url, save_func, depth, max_depth)\n",
    "                visited_urls.add(next_url)\n",
    "                page_count += 1\n",
    "\n",
    "await main(\"https://lespotevry.fr\", max_pages=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "\n",
    "documents = text_splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[1;32m      4\u001b[0m embedding_model \u001b[38;5;241m=\u001b[39m OpenAIEmbeddings()\n\u001b[0;32m----> 5\u001b[0m db \u001b[38;5;241m=\u001b[39m Chroma\u001b[38;5;241m.\u001b[39mfrom_documents(\u001b[43mdocuments\u001b[49m, embedding_model, persist_directory\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../db/chroma/HS-01\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m db\u001b[38;5;241m.\u001b[39mpersist()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'documents' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "db = Chroma.from_documents(documents, embedding_model, persist_directory=\"../db/chroma/HS-01\")\n",
    "db.persist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
