{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65880853-35c5-4d39-8b60-15e4e9ef1a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48d34513-63f1-4aaa-816b-ecdf6813e115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from typing import (List)\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "splitter = RecursiveCharacterTextSplitter()\n",
    "\n",
    "documents = []\n",
    "loaders = [\n",
    "    PyPDFLoader(\"../data/autogpt.pdf\"),\n",
    "    PyPDFLoader(\"../data/lora.pdf\"),\n",
    "]\n",
    "\n",
    "for loader in loaders:\n",
    "    documents.extend(loader.load())\n",
    "\n",
    "chunks = splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7767714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_community.vectorstores.chroma.Chroma object at 0x10e046f90>\n"
     ]
    }
   ],
   "source": [
    "persist_directory = '../docs/chroma/02'\n",
    "vectordb = Chroma(\n",
    "    embedding_function=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "\n",
    "if (len(vectordb.get()['ids']) < 1):\n",
    "    print('persist...')\n",
    "    vectordb.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embedding,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "\n",
    "\n",
    "print(vectordb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30dbca94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The LoRA paper introduces an efficient adaptation strategy for large language models that allows for quick task-switching without introducing inference latency or reducing input sequence length. By adapting only a subset of weight matrices in a neural network, LoRA reduces memory and storage usage significantly, making it more cost-effective to deploy customized models for different tasks. The paper also explores the impact of LoRA on downstream task performance for various language models like RoBERTa, DeBERTa, and GPT-2, showcasing its effectiveness in improving model quality while reducing training time and resource requirements. Thanks for asking!\n",
      "The additional context provided includes hyperparameters for DeBERTa XXL on tasks in the GLUE benchmark, hyperparameters for GPT-2 LoRA on E2E, WebNLG, and DART datasets, and details on parameter tuning for LoRA in combination with preﬁx-based approaches on WikiSQL and MNLI tasks. The hyperparameters include optimizer, learning rate, weight decay, dropout probability, batch size, number of epochs, warmup steps, and other settings specific to each dataset and model.\n",
      "\n",
      "The hyperparameters play a crucial role in training and fine-tuning large language models like DeBERTa XXL and GPT-2 LoRA, impacting model performance and efficiency. The combination of LoRA with preﬁx-based approaches, such as preﬁx-embedding tuning and preﬁx-layer tuning, aims to further enhance model capabilities on specific tasks like WikiSQL and MNLI.\n",
      "\n",
      "Overall, the detailed hyperparameter settings and tuning strategies highlight the importance of optimizing model configurations for different datasets and tasks to achieve the best performance.\n",
      "The LoRA paper introduces a technique called Low-Rank Adaptation, which aims to streamline the fine-tuning process for large language models like Transformers. By selectively adjusting key parameters while sharing most model weights, LoRA reduces memory usage during training, leading to more efficient resource utilization and faster task-switching. Empirical experiments on models such as RoBERTa and GPT-2 showcase LoRA's effectiveness in improving performance on natural language understanding and generation tasks. The paper emphasizes the correlation between the adaptation matrix and the pre-trained model, highlighting how LoRA amplifies important but overlooked features to enhance model performance without compromising input sequence length. Additionally, the integration of LoRA with prefix-based tuning approaches like LoRA+PE and LoRA+PL further boosts model capabilities on tasks like WikiSQL and MNLI. Overall, LoRA presents a promising strategy for optimizing neural network training and deployment processes by emphasizing crucial features for task-specific performance while reducing complexity and cost. Future directions include exploring additional efficient adaptation methods and extending the application of LoRA to various neural networks beyond language models.\n",
      "I don't know. Thanks for asking!\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Build prompt\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")\n",
    "\n",
    "refine_qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    chain_type='refine',\n",
    "    return_source_documents=True,\n",
    ")\n",
    "\n",
    "map_qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    chain_type='map_reduce',\n",
    "    return_source_documents=True,\n",
    ")\n",
    "\n",
    "query = 'Make a summary in 200 words of lora paper' \n",
    "result = qa_chain({\"query\": query})\n",
    "print(result[\"result\"])\n",
    "\n",
    "\n",
    "query = 'Make a summary in 200 words of lora paper' \n",
    "result = refine_qa_chain({\"query\": query})\n",
    "print(result[\"result\"])\n",
    "\n",
    "query = 'Make a summary in 200 words of lora paper' \n",
    "result = map_qa_chain({\"query\": query})\n",
    "print(result[\"result\"])\n",
    "\n",
    "\n",
    "\n",
    "query = 'What  is the biggest country in the world' \n",
    "result = qa_chain({\"query\": query})\n",
    "print(result[\"result\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2a3f84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
